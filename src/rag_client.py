import logging
import asyncio
import json
from typing import Dict, Any, Optional, AsyncGenerator
from dotenv import load_dotenv
import os
from src.llm_client import LLMClient
from src.utils.prompt_builder import PromptBuilder
from src.retrieval_strategies import VectorRetrievalStrategy, LLMRetrievalStrategy, RetrievalStrategy

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

logger = logging.getLogger(__name__)

class RAGClient:
    """RAG (Retrieval-Augmented Generation) å®¢æˆ¶ç«¯ï¼Œæ”¯æ´å‘é‡æª¢ç´¢å’Œ LLM æª¢ç´¢ï¼Œæ”¯æ´ streaming æ¨¡å¼"""
    
    def __init__(self):
        """
        åˆå§‹åŒ– RAG å®¢æˆ¶ç«¯
        """
        # åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯
        self.llm_client = LLMClient()
    
    async def _classify_question(self, user_question: str) -> str:
        """
        åˆ†é¡ç”¨æˆ¶å•é¡Œé¡å‹
        
        Args:
            user_question: ç”¨æˆ¶å•é¡Œ
            
        Returns:
            å•é¡Œé¡å‹ ("meta_question" æˆ– "medical_question")
        """
        try:
            classifier_prompt = PromptBuilder.build_question_classifier_prompt(user_question)
            
            messages = [
                {
                    "role": "system",
                    "content": PromptBuilder.get_system_prompt("question_classifier")
                },
                {
                    "role": "user",
                    "content": classifier_prompt
                }
            ]
            
            logger.info("é–‹å§‹åˆ†é¡å•é¡Œé¡å‹...")
            logger.info(f"ğŸ” [DEBUG] åˆ†é¡å™¨è¼¸å…¥å•é¡Œ: {user_question}")
            
            # åˆ†é¡å™¨ä½¿ç”¨è¼ƒä½çš„ temperature å’Œè¼ƒå°‘çš„ tokensï¼Œå› ç‚ºåªéœ€è¦è¿”å› JSON
            raw_response = await self.llm_client.generate_response(
                messages,
                max_tokens=50,  # åˆ†é¡å™¨åªéœ€è¦å¾ˆå°‘çš„ tokens
                temperature=0.1  # ä½ temperature ç¢ºä¿ç©©å®šçš„åˆ†é¡çµæœ
            )
            
            logger.info(f"ğŸ” [DEBUG] åˆ†é¡å™¨åŸå§‹éŸ¿æ‡‰: {repr(raw_response)}")
            
            # è§£æ JSON éŸ¿æ‡‰
            try:
                # å˜—è©¦æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç¢¼å¡Šæˆ–å…¶ä»–æ–‡æœ¬ï¼‰
                cleaned_response = raw_response.strip()
                logger.info(f"ğŸ” [DEBUG] æ¸…ç†å¾Œçš„éŸ¿æ‡‰: {repr(cleaned_response)}")
                
                # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
                if cleaned_response.startswith("```"):
                    lines = cleaned_response.split("\n")
                    cleaned_response = "\n".join(lines[1:-1]) if len(lines) > 2 else cleaned_response
                    logger.info(f"ğŸ” [DEBUG] ç§»é™¤ markdown ä»£ç¢¼å¡Šå¾Œ: {repr(cleaned_response)}")
                elif "```json" in cleaned_response:
                    cleaned_response = cleaned_response.split("```json")[1].split("```")[0].strip()
                    logger.info(f"ğŸ” [DEBUG] æå– JSON ä»£ç¢¼å¡Šå¾Œ: {repr(cleaned_response)}")
                
                result = json.loads(cleaned_response)
                logger.info(f"ğŸ” [DEBUG] è§£æå¾Œçš„ JSON çµæœ: {result}")
                
                question_type = result.get("question_type", "medical_question")
                logger.info(f"âœ… å•é¡Œåˆ†é¡çµæœ: {question_type}")
                return question_type
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ ç„¡æ³•è§£æåˆ†é¡å™¨ JSON éŸ¿æ‡‰")
                logger.warning(f"ğŸ” [DEBUG] åŸå§‹éŸ¿æ‡‰: {repr(raw_response)}")
                if 'cleaned_response' in locals():
                    logger.warning(f"ğŸ” [DEBUG] æ¸…ç†å¾ŒéŸ¿æ‡‰: {repr(cleaned_response)}")
                logger.warning(f"ğŸ” [DEBUG] JSON è§£æéŒ¯èª¤: {e}")
                # å¦‚æœç„¡æ³•è§£æï¼Œå˜—è©¦å¾æ–‡æœ¬ä¸­æå–
                if "meta_question" in raw_response.lower():
                    logger.info("âœ… å¾æ–‡æœ¬ä¸­æå–åˆ° meta_questionï¼Œè¿”å› meta_question")
                    return "meta_question"
                logger.info("âœ… ç„¡æ³•ç¢ºå®šé¡å‹ï¼Œé»˜èªè¿”å› medical_question")
                return "medical_question"  # é»˜èªè¿”å›é†«ç™‚å•é¡Œ
                
        except Exception as e:
            logger.error(f"åˆ†é¡å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # ç™¼ç”ŸéŒ¯èª¤æ™‚é»˜èªè¿”å›é†«ç™‚å•é¡Œï¼Œç¢ºä¿ç³»çµ±ç¹¼çºŒé‹è¡Œ
            return "medical_question"
    
    async def retrieve_relevant_context(self, retrieval_strategy: RetrievalStrategy, user_question: str, database_content: Dict[str, Any]) -> str:
        """
        å¾è³‡æ–™åº«ä¸­æª¢ç´¢ç›¸é—œå…§å®¹
        
        Args:
            retrieval_strategy: The retrieval strategy to use.
            user_question: ç”¨æˆ¶å•é¡Œ
            database_content: è³‡æ–™åº«å…§å®¹
            
        Returns:
            æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹
        """
        if asyncio.iscoroutinefunction(retrieval_strategy.retrieve):
            return await retrieval_strategy.retrieve(user_question, database_content)
        else:
            return retrieval_strategy.retrieve(user_question, database_content)
    
    async def enhance_response_with_rag_stream(self, user_question: str, fhir_data: str, 
                                             database_content: Dict[str, Any], retrieval_type: str, conversation_history: str = "",
                                             require_retrieval: bool = False, enable_conversation_history: bool = True) -> AsyncGenerator[str, None]:
        """
        ä½¿ç”¨ RAG å¢å¼·å›æ‡‰ï¼ˆstreaming æ¨¡å¼ï¼‰
        
        æµç¨‹ï¼š
        1. ç¬¬ä¸€å±¤ï¼šå•é¡Œåˆ†é¡å™¨åˆ¤æ–·å•é¡Œé¡å‹
        2. ç¬¬äºŒå±¤ï¼šæ ¹æ“šåˆ†é¡çµæœé¸æ“‡ä¸åŒçš„è™•ç†æµç¨‹
           - meta_question: ä½¿ç”¨å…ƒå•é¡Œå°ˆç”¨ promptï¼ˆè·³éæª¢ç´¢ï¼‰
           - medical_question: ä½¿ç”¨ç¾æœ‰çš„ RAG æµç¨‹
        
        Args:
            user_question: ç”¨æˆ¶å•é¡Œ
            fhir_data: FHIR è³‡æ–™
            database_content: è³‡æ–™åº«å…§å®¹
            retrieval_type: The retrieval type to use.
            conversation_history: å°è©±æ­·å²ï¼ˆå¯é¸ï¼‰
            require_retrieval: æ˜¯å¦å¿…é ˆæª¢ç´¢åˆ°è³‡æ–™ï¼ˆè‹¥ç‚º True ä¸”æœªæª¢ç´¢åˆ°è³‡æ–™å‰‡å›å ±éŒ¯èª¤ï¼‰
            enable_conversation_history: æ˜¯å¦å•Ÿç”¨æ­·å²å°è©±åŠŸèƒ½ï¼ˆè‹¥ç‚º False ä¸”æœªæª¢ç´¢åˆ°è³‡æ–™å‰‡å›å ±éŒ¯èª¤ï¼‰
            
        Yields:
            å¢å¼·å›æ‡‰çš„æ–‡å­—ç‰‡æ®µ
        """
        try:
            # =============================
            # ç¬¬ä¸€å±¤ï¼šå•é¡Œåˆ†é¡å™¨
            # =============================
            question_type = await self._classify_question(user_question)
            logger.info(f"å•é¡Œåˆ†é¡çµæœ: {question_type}")
            
            # =============================
            # ç¬¬äºŒå±¤ï¼šæ ¹æ“šåˆ†é¡çµæœè·¯ç”±
            # =============================
            if question_type == "meta_question":
                # å…ƒå•é¡Œï¼šè·³éæª¢ç´¢ï¼Œç›´æ¥ä½¿ç”¨ FHIR æ•¸æ“šç”Ÿæˆå›æ‡‰
                logger.info("æª¢æ¸¬åˆ°å…ƒå•é¡Œï¼Œä½¿ç”¨å…ƒå•é¡Œå°ˆç”¨è™•ç†æµç¨‹")
                async for chunk in self._generate_meta_question_response_stream(fhir_data):
                    yield chunk
                return
            
            # é†«ç™‚å•é¡Œï¼šä½¿ç”¨ç¾æœ‰çš„ RAG æµç¨‹
            logger.info("æª¢æ¸¬åˆ°é†«ç™‚å•é¡Œï¼Œä½¿ç”¨æ¨™æº– RAG è™•ç†æµç¨‹")
            
            # æ ¹æ“šæª¢ç´¢é¡å‹è¨­å®š RAG å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨ä¾è³´æ³¨å…¥é¿å…é‡è¤‡å‰µå»º LLMClientï¼‰
            if retrieval_type == "llm":
                retrieval_strategy = LLMRetrievalStrategy(self.llm_client)
            else:
                retrieval_strategy = VectorRetrievalStrategy()

            # æª¢ç´¢ç›¸é—œå…§å®¹
            retrieved_context = await self.retrieve_relevant_context(retrieval_strategy, user_question, database_content)
            
            if not retrieved_context or retrieved_context.strip() == "No relevant content retrieved.":
                # å¦‚æœè¨­å®šå¿…é ˆæª¢ç´¢ï¼Œå‰‡å›å ±éŒ¯èª¤
                if require_retrieval:
                    logger.warning("æœªæ‰¾åˆ°ç›¸é—œçš„è³‡æ–™åº«å…§å®¹ï¼Œä¸”è¨­å®šç‚ºå¿…é ˆæª¢ç´¢æ¨¡å¼")
                    yield "éŒ¯èª¤ï¼šæœªèƒ½å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹ç¢ºèªæ‚¨çš„å•é¡Œæ˜¯å¦èˆ‡çŸ¥è­˜åº«å…§å®¹ç›¸é—œï¼Œæˆ–å˜—è©¦èª¿æ•´å•é¡Œçš„è¡¨è¿°æ–¹å¼ã€‚"
                    return
                
                # å¦‚æœæ­·å²å°è©±åŠŸèƒ½å·²åœç”¨ï¼Œå‰‡å¿…é ˆè¦æœ‰æª¢ç´¢è³‡æ–™æ‰èƒ½å›æ‡‰
                if not enable_conversation_history:
                    logger.warning("æœªæ‰¾åˆ°ç›¸é—œçš„è³‡æ–™åº«å…§å®¹ï¼Œä¸”æ­·å²å°è©±åŠŸèƒ½å·²åœç”¨ï¼Œç„¡æ³•ç”Ÿæˆå›æ‡‰")
                    yield "éŒ¯èª¤ï¼šæœªèƒ½å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢åˆ°ç›¸é—œè³‡æ–™ã€‚ç”±æ–¼æ­·å²å°è©±åŠŸèƒ½å·²åœç”¨ï¼Œç³»çµ±éœ€è¦æª¢ç´¢è³‡æ–™æ‰èƒ½å›æ‡‰ã€‚è«‹ç¢ºèªæ‚¨çš„å•é¡Œæ˜¯å¦èˆ‡çŸ¥è­˜åº«å…§å®¹ç›¸é—œï¼Œæˆ–å˜—è©¦èª¿æ•´å•é¡Œçš„è¡¨è¿°æ–¹å¼ã€‚"
                    return
                
                # æ­·å²å°è©±åŠŸèƒ½å·²å•Ÿç”¨ï¼Œå¯ä»¥ä½¿ç”¨æ­·å²å°è©±ç”Ÿæˆå›æ‡‰
                logger.info("æœªæ‰¾åˆ°ç›¸é—œçš„è³‡æ–™åº«å…§å®¹ï¼Œå°‡ä½¿ç”¨æ­·å²å°è©±ç”Ÿæˆå›æ‡‰")
                async for chunk in self._generate_base_response_stream(user_question, fhir_data, conversation_history):
                    yield chunk
                return
            
            # æ ¹æ“šæ˜¯å¦æœ‰æ­·å²å°è©±é¸æ“‡ç”Ÿæˆæ–¹æ³•
            if conversation_history and conversation_history.strip() != "None":
                # æœ‰æ­·å²å°è©±ï¼Œä½¿ç”¨å¢å¼·å›æ‡‰
                async for chunk in self._generate_enhanced_response_stream(user_question, fhir_data, retrieved_context, conversation_history):
                    yield chunk
            else:
                # ç„¡æ­·å²å°è©±ï¼Œåƒ…ä½¿ç”¨æª¢ç´¢è³‡æ–™
                async for chunk in self._generate_retrieval_only_response_stream(user_question, fhir_data, retrieved_context):
                    yield chunk
        
        except asyncio.CancelledError:
            logger.info("RAG å¢å¼·å›æ‡‰è¢«å–æ¶ˆ")
            raise
                
        except Exception as e:
            logger.error(f"å¢å¼·å›æ‡‰ï¼ˆstreamingï¼‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            yield f"ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    async def _generate_base_response_stream(self, user_question: str, fhir_data: str, conversation_history: str = "") -> AsyncGenerator[str, None]:
        """ç”ŸæˆåŸºç¤å›æ‡‰ï¼ˆstreaming æ¨¡å¼ï¼‰"""
        base_prompt = PromptBuilder.build_base_prompt(user_question, fhir_data, conversation_history)
        
        messages = [
            {
                "role": "system",
                "content": PromptBuilder.get_system_prompt("base")
            },
            {
                "role": "user",
                "content": base_prompt
            }
        ]
        
        logger.info("é–‹å§‹ç”ŸæˆåŸºç¤å›æ‡‰ï¼ˆstreamingï¼‰...")
        async for chunk in self.llm_client.generate_response_stream(
            messages, 
            max_tokens=int(os.getenv("LLM_DEFAULT_MAX_TOKENS", 2000)), 
            temperature=float(os.getenv("LLM_ENHANCEMENT_TEMPERATURE", 0.3))
        ):
            yield chunk
    
    async def _generate_enhanced_response_stream(self, user_question: str, fhir_data: str, 
                                               retrieved_context: str, conversation_history: str = "") -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå¢å¼·å›æ‡‰ï¼ˆstreaming æ¨¡å¼ï¼‰"""
        enhancement_prompt = PromptBuilder.build_enhancement_prompt(
            user_question, fhir_data, retrieved_context, conversation_history
        )

        # print(f"Enhancement Prompt: {enhancement_prompt}")  # Debugging line
        
        messages = [
            {
                "role": "system",
                "content": PromptBuilder.get_system_prompt("enhancement")
            },
            {
                "role": "user",
                "content": enhancement_prompt
            }
        ]
        
        logger.info("é–‹å§‹ç”Ÿæˆå¢å¼·å›æ‡‰ï¼ˆstreamingï¼‰...")
        async for chunk in self.llm_client.generate_response_stream(
            messages, 
            max_tokens=int(os.getenv("LLM_DEFAULT_MAX_TOKENS", 2000)), 
            temperature=float(os.getenv("LLM_ENHANCEMENT_TEMPERATURE", 0.3))
        ):
            yield chunk
    
    async def _generate_retrieval_only_response_stream(self, user_question: str, fhir_data: str, 
                                                      retrieved_context: str) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆåƒ…åŸºæ–¼æª¢ç´¢è³‡æ–™çš„å›æ‡‰ï¼ˆç„¡æ­·å²å°è©±ï¼Œstreaming æ¨¡å¼ï¼‰"""
        retrieval_only_prompt = PromptBuilder.build_retrieval_only_prompt(
            user_question, fhir_data, retrieved_context
        )
        
        messages = [
            {
                "role": "system",
                "content": PromptBuilder.get_system_prompt("retrieval_only")
            },
            {
                "role": "user",
                "content": retrieval_only_prompt
            }
        ]
        
        logger.info("é–‹å§‹ç”Ÿæˆåƒ…æª¢ç´¢è³‡æ–™å›æ‡‰ï¼ˆç„¡æ­·å²å°è©±ï¼Œstreamingï¼‰...")
        async for chunk in self.llm_client.generate_response_stream(
            messages, 
            max_tokens=int(os.getenv("LLM_DEFAULT_MAX_TOKENS", 2000)), 
            temperature=float(os.getenv("LLM_ENHANCEMENT_TEMPERATURE", 0.3))
        ):
            yield chunk
    
    async def _generate_meta_question_response_stream(self, fhir_data: str) -> AsyncGenerator[str, None]:
        """
        ç”Ÿæˆå…ƒå•é¡Œå›æ‡‰ï¼ˆstreaming æ¨¡å¼ï¼‰
        ç”¨æ–¼å›ç­” "what questions can you answer?" ç­‰å•é¡Œ
        
        Args:
            fhir_data: FHIR è³‡æ–™
            
        Yields:
            å›æ‡‰çš„æ–‡å­—ç‰‡æ®µ
        """
        meta_question_prompt = PromptBuilder.build_meta_question_prompt(fhir_data)
        
        messages = [
            {
                "role": "system",
                "content": PromptBuilder.get_system_prompt("meta_question")
            },
            {
                "role": "user",
                "content": meta_question_prompt
            }
        ]
        
        logger.info("é–‹å§‹ç”Ÿæˆå…ƒå•é¡Œå›æ‡‰ï¼ˆstreamingï¼‰...")
        async for chunk in self.llm_client.generate_response_stream(
            messages,
            max_tokens=int(os.getenv("LLM_DEFAULT_MAX_TOKENS", 2000)),
            temperature=float(os.getenv("LLM_ENHANCEMENT_TEMPERATURE", 0.3))
        ):
            yield chunk 